# master_crawler.py
import os
import json
import time
import schedule
import shutil
import glob
from datetime import datetime
from iea_crawler import IEASolarContentCrawler
from pv_magazine_crawler import PVMagazineSeleniumCrawler
from irena_crawler import IrenaCrawler
from combined_crawler import CombinedSolarCrawler
from translator import MultiFileTranslator

def save_individual_crawler_data(crawler_name, data, output_dir="output/individual"):
    """ä¿å­˜å•ä¸ªçˆ¬è™«çš„æ•°æ®åˆ°ç‹¬ç«‹æ–‡ä»¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{crawler_name}_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"{crawler_name}æ•°æ®å·²ä¿å­˜: {filepath}")
    return filepath

def cleanup_chrome_temp():
    """æ¸…ç†Chromeä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å¤šå®ä¾‹å†²çª"""
    try:
        import tempfile
        temp_dir = tempfile.gettempdir()

        # æ¸…ç†Chromeç›¸å…³çš„ä¸´æ—¶ç›®å½•
        patterns = [
            os.path.join(temp_dir, 'chrome_*'),
            os.path.join(temp_dir, '.com.google.Chrome.*'),
            os.path.join(temp_dir, 'scoped_dir*')
        ]

        for pattern in patterns:
            for path in glob.glob(pattern):
                try:
                    if os.path.isdir(path):
                        shutil.rmtree(path, ignore_errors=True)
                except:
                    pass

        print("ğŸ§¹ å·²æ¸…ç†Chromeä¸´æ—¶æ–‡ä»¶")
    except Exception as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")

# åœ¨master_crawler.pyçš„run_all_crawlerså‡½æ•°ä¸­ä¿®æ”¹ï¼š

def run_all_crawlers():
    """è¿è¡Œæ‰€æœ‰çˆ¬è™«å¹¶åˆ†åˆ«ä¿å­˜ç»“æœ"""
    all_data = []
    individual_files = {}
    
    try:
        print(f"\n=== å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
        
        # 1. è¿è¡ŒIEAçˆ¬è™«
        print("\n=== å¼€å§‹è¿è¡ŒIEAçˆ¬è™« ===")
        iea_crawler = IEASolarContentCrawler()
        iea_crawler.search_solar_content()
        iea_file = save_individual_crawler_data("iea", iea_crawler.content_data)
        individual_files["iea"] = iea_file
        all_data.extend(iea_crawler.content_data)
        print(f"IEAçˆ¬è™«å®Œæˆï¼Œè·å– {len(iea_crawler.content_data)} æ¡æ•°æ®")

        # ç­‰å¾…å¹¶æ¸…ç†ï¼Œé¿å…å†²çª
        print("â³ ç­‰å¾…10ç§’ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        time.sleep(10)
        cleanup_chrome_temp()

        # 2. è¿è¡ŒPV Magazineçˆ¬è™«
        print("\n=== å¼€å§‹è¿è¡ŒPV Magazineçˆ¬è™« ===")
        try:
            pvmagazine_crawler = PVMagazineSeleniumCrawler()
            pvmagazine_crawler.search_solar_content()
            pvmagazine_file = save_individual_crawler_data("pvmagazine", pvmagazine_crawler.content_data)
            individual_files["pvmagazine"] = pvmagazine_file
            all_data.extend(pvmagazine_crawler.content_data)
            pvmagazine_crawler.close()
            print(f"PV Magazineçˆ¬è™«å®Œæˆï¼Œè·å– {len(pvmagazine_crawler.content_data)} æ¡æ•°æ®")
        except Exception as e:
            print(f"PV Magazineçˆ¬è™«å¤±è´¥: {e}")
            print("è·³è¿‡PV Magazineçˆ¬è™«ï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–çˆ¬è™«...")

        # ç­‰å¾…å¹¶æ¸…ç†ï¼Œé¿å…å†²çª
        print("â³ ç­‰å¾…10ç§’ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        time.sleep(10)
        cleanup_chrome_temp()

        # 3. è¿è¡ŒIRENAçˆ¬è™«ï¼ˆä¿®å¤æ–¹æ³•åï¼‰
        print("\n=== å¼€å§‹è¿è¡ŒIRENAçˆ¬è™« ===")
        try:
            irena_crawler = IrenaCrawler()
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            if hasattr(irena_crawler, 'crawl_with_load_more'):
                news_data = irena_crawler.crawl_with_load_more(loads_per_keyword=3)
                # å°†æ•°æ®æ ¼å¼ç»Ÿä¸€ä¸ºcontent_data
                irena_crawler.content_data = news_data
            elif hasattr(irena_crawler, 'search_solar_content'):
                irena_crawler.search_solar_content()
            else:
                print("IRENAçˆ¬è™«æ²¡æœ‰å¯ç”¨çš„çˆ¬å–æ–¹æ³•")
                news_data = []
            
            irena_file = save_individual_crawler_data("irena", irena_crawler.content_data)
            individual_files["irena"] = irena_file
            all_data.extend(irena_crawler.content_data)
            print(f"IRENAçˆ¬è™«å®Œæˆï¼Œè·å– {len(irena_crawler.content_data)} æ¡æ•°æ®")
        except Exception as e:
            print(f"IRENAçˆ¬è™«å¤±è´¥: {e}")
            print("è·³è¿‡IRENAçˆ¬è™«ï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–çˆ¬è™«...")

        # ç­‰å¾…å¹¶æ¸…ç†ï¼Œé¿å…å†²çª
        print("â³ ç­‰å¾…10ç§’ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        time.sleep(10)
        cleanup_chrome_temp()

        # 4. è¿è¡ŒCombinedçˆ¬è™«ï¼ˆä¿®å¤æ–¹æ³•åï¼‰
        print("\n=== å¼€å§‹è¿è¡ŒCombinedçˆ¬è™« ===")
        try:
            combined_crawler = CombinedSolarCrawler()
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            if hasattr(combined_crawler, 'get_news_data'):
                news_data = combined_crawler.get_news_data(pages=3)
                # å°†æ•°æ®æ ¼å¼ç»Ÿä¸€ä¸ºcontent_data
                combined_crawler.content_data = news_data
            elif hasattr(combined_crawler, 'search_solar_content'):
                combined_crawler.search_solar_content()
            else:
                print("Combinedçˆ¬è™«æ²¡æœ‰å¯ç”¨çš„çˆ¬å–æ–¹æ³•")
                news_data = []
            
            combined_file = save_individual_crawler_data("combined", combined_crawler.content_data)
            individual_files["combined"] = combined_file
            all_data.extend(combined_crawler.content_data)
            print(f"Combinedçˆ¬è™«å®Œæˆï¼Œè·å– {len(combined_crawler.content_data)} æ¡æ•°æ®")
        except Exception as e:
            print(f"Combinedçˆ¬è™«å¤±è´¥: {e}")
            print("è·³è¿‡Combinedçˆ¬è™«...")

        # æœ€åæ¸…ç†
        print("â³ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleanup_chrome_temp()

        print(f"\n=== æ‰€æœ‰çˆ¬è™«ä»»åŠ¡å®Œæˆ ===")
        print(f"æ€»è®¡è·å– {len(all_data)} æ¡å†…å®¹")
        print("å„çˆ¬è™«æ–‡ä»¶:")
        for name, filepath in individual_files.items():
            print(f"  - {name}: {filepath}")

        # 5. è¿è¡Œç¿»è¯‘
        print("\n=== å¼€å§‹ç¿»è¯‘ ===")
        try:
            translator = MultiFileTranslator()
            output_file = translator.merge_and_save_translations()
            if output_file:
                print(f"âœ… ç¿»è¯‘å®Œæˆï¼Œæ–‡ä»¶: {output_file}")
            else:
                print("âš ï¸ ç¿»è¯‘æœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
        except Exception as e:
            print(f"ç¿»è¯‘å¤±è´¥: {e}")

        return {
            "individual_files": individual_files,
            "total_count": len(all_data)
        }
        
    except Exception as e:
        print(f"çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        return {}

def setup_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ - æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ç”¨äºæµ‹è¯•"""
    # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼ˆæµ‹è¯•ç”¨ï¼‰
    schedule.every().hour.do(run_all_crawlers)
    
    print("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å°æ—¶è‡ªåŠ¨è¿è¡Œï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
    print("ç¨‹åºæŒç»­è¿è¡Œä¸­...")

def run_scheduler():
    """è¿è¡Œè°ƒåº¦å™¨"""
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "now":
            # ç«‹å³è¿è¡Œä¸€æ¬¡
            run_all_crawlers()
        elif sys.argv[1] == "daily":
            # è®¾ç½®ä¸ºæ¯æ—¥è¿è¡Œæ¨¡å¼
            schedule.clear()
            schedule.every().day.at("09:00").do(run_all_crawlers)
            print("å·²è®¾ç½®ä¸ºæ¯æ—¥ä¸Šåˆ9ç‚¹è¿è¡Œæ¨¡å¼")
            run_scheduler()
        else:
            print("Usage:")
            print("  python master_crawler.py now     # ç«‹å³è¿è¡Œä¸€æ¬¡")
            print("  python master_crawler.py daily   # è®¾ç½®ä¸ºæ¯æ—¥è¿è¡Œ")
            print("  python master_crawler.py         # é»˜è®¤æ¯å°æ—¶è¿è¡Œï¼ˆæµ‹è¯•ï¼‰")
    else:
        # é»˜è®¤ï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å°æ—¶è¿è¡Œï¼Œæµ‹è¯•ç”¨ï¼‰
        setup_scheduler()
        run_scheduler()